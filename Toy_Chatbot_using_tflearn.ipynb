{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Toy Chatbot using tflearn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VXBjLY-ZkEp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a1828d9d-d94b-428f-ea25-e5b5a12fb1e8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw8jDYAaa9j2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "1900bd0c-abe1-4e74-8ba3-b5095ee84e92"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import random\n",
        "import json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0820 19:10:37.178113 139627540498304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0820 19:10:37.179788 139627540498304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0820 19:10:37.197487 139627540498304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0820 19:10:37.206025 139627540498304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W0820 19:10:37.219600 139627540498304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0820 19:10:37.220663 139627540498304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEzz3MT5bBRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkPRNRA5bMqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "outputId": "7b7901a3-b22e-4845-bb33-4c5059ef2a55"
      },
      "source": [
        "intents"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intents': [{'context_set': '',\n",
              "   'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day'],\n",
              "   'responses': ['Hello, thanks for visiting',\n",
              "    'Good to see you again',\n",
              "    'Hi there, how can I help?'],\n",
              "   'tag': 'greeting'},\n",
              "  {'patterns': ['Bye', 'See you later', 'Goodbye'],\n",
              "   'responses': ['See you later, thanks for visiting',\n",
              "    'Have a nice day',\n",
              "    'Bye! Come back again soon.'],\n",
              "   'tag': 'goodbye'},\n",
              "  {'patterns': ['Thanks', 'Thank you', \"That's helpful\"],\n",
              "   'responses': ['Happy to help!', 'Any time!', 'My pleasure'],\n",
              "   'tag': 'thanks'},\n",
              "  {'patterns': ['What hours are you open?',\n",
              "    'What are your hours?',\n",
              "    'When are you open?'],\n",
              "   'responses': [\"We're open every day 9am-9pm\",\n",
              "    'Our hours are 9am-9pm every day'],\n",
              "   'tag': 'hours'},\n",
              "  {'patterns': ['What is your location?',\n",
              "    'Where are you located?',\n",
              "    'What is your address?',\n",
              "    'Where is your restaurant situated?'],\n",
              "   'responses': ['We are on the intersection of London Alley and Bridge Avenue.',\n",
              "    'We are situated at the intersection of London Alley and Bridge Avenue',\n",
              "    'Our Address is: 1000 Bridge Avenue, London EC3N 4AJ, UK'],\n",
              "   'tag': 'location'},\n",
              "  {'patterns': ['Do you take credit cards?',\n",
              "    'Do you accept Mastercard?',\n",
              "    'Are you cash only?'],\n",
              "   'responses': ['We accept VISA, Mastercard and AMEX',\n",
              "    'We accept most major credit cards'],\n",
              "   'tag': 'payments'},\n",
              "  {'patterns': ['What is your menu for today?',\n",
              "    'What are you serving today?',\n",
              "    \"What is today's special?\"],\n",
              "   'responses': [\"Today's special is Chicken Tikka\",\n",
              "    'Our speciality for today is Chicken Tikka'],\n",
              "   'tag': 'todaysmenu'},\n",
              "  {'context_set': 'food',\n",
              "   'patterns': ['Do you provide home delivery?',\n",
              "    'Do you deliver the food?',\n",
              "    'What are the home delivery options?'],\n",
              "   'responses': ['Yes, we provide home delivery through UBER Eats and Zomato?',\n",
              "    'We have home delivery options through UBER Eats and Zomato'],\n",
              "   'tag': 'deliveryoption'},\n",
              "  {'context_filter': 'food',\n",
              "   'patterns': ['What is your Menu?',\n",
              "    'What are the main course options?',\n",
              "    'Can you tell me the most delicious dish from the menu?',\n",
              "    \"What is the today's special?\"],\n",
              "   'responses': ['You can visit www.mymenu.com for menu options',\n",
              "    'You can check out the food menu at www.mymenu.com',\n",
              "    'You can check various delicacies given in the food menu at www.mymenu.com'],\n",
              "   'tag': 'menu'}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zYGKs7IbPuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore = ['?']\n",
        "# loop through each sentence in the intent's patterns\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each and every word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add word to the words list\n",
        "        words.extend(w)\n",
        "        # add word(s) to documents\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add tags to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWpDnE2VbeTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a813df1f-98ed-43a3-c7a9-d558b09cd940"
      },
      "source": [
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicate classes\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31 documents\n",
            "9 classes ['deliveryoption', 'goodbye', 'greeting', 'hours', 'location', 'menu', 'payments', 'thanks', 'todaysmenu']\n",
            "57 unique stemmed words [\"'s\", 'acceiv', 'address', 'anyon', 'ar', 'bye', 'can', 'card', 'cash', 'cours', 'credit', 'day', 'del', 'delicy', 'delivery', 'dish', 'do', 'food', 'for', 'from', 'good', 'goodby', 'hello', 'help', 'hi', 'hom', 'hour', 'how', 'is', 'lat', 'loc', 'main', 'mastercard', 'me', 'menu', 'most', 'on', 'op', 'opt', 'provid', 'resta', 'see', 'serv', 'situ', 'spec', 'tak', 'tel', 'thank', 'that', 'the', 'ther', 'today', 'what', 'when', 'wher', 'yo', 'you']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRWneyuibt6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create training data\n",
        "training = []\n",
        "output = []\n",
        "# create an empty array for output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# create training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stemming each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # create bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is '1' for current tag and '0' for rest of other tags\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffling features and turning it into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP5-syorcU2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating training lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o4cAA4-ca7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resetting underlying graph data\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kce5oU13cfm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building neural network\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
        "net = tflearn.regression(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE-ySkSLci-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "27d45172-bbce-4904-e9c9-f4cd1f6759f5"
      },
      "source": [
        "# Defining model and setting up tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0820 19:13:19.778476 139627540498304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0820 19:13:19.856512 139627540498304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0820 19:13:20.019215 139627540498304 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rvq11Lmcpk9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "61324c07-1b74-4d4a-9984-9637563a8631"
      },
      "source": [
        "# Start training\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save('modelc.h5')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Step: 4399  | total loss: \u001b[1m\u001b[32m0.17185\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 1100 | loss: 0.17185 - acc: 0.9873 -- iter: 24/31\n",
            "Training Step: 4400  | total loss: \u001b[1m\u001b[32m0.16178\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 1100 | loss: 0.16178 - acc: 0.9885 -- iter: 31/31\n",
            "--\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0820 19:18:31.065297 139627540498304 meta_graph.py:449] Issue encountered when serializing data_preprocessing.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'NoneType' object has no attribute 'name'\n",
            "W0820 19:18:31.066840 139627540498304 meta_graph.py:449] Issue encountered when serializing data_augmentation.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'NoneType' object has no attribute 'name'\n",
            "W0820 19:18:31.072033 139627540498304 meta_graph.py:449] Issue encountered when serializing summary_tags.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'dict' object has no attribute 'name'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCPMCISydjuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMDR2uTVd_JA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# restoring all the data structures\n",
        "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpRvILm3eDOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenizing the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stemming each word\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHQSLxNeeI-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returning bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words, show_details=False):\n",
        "    # tokenizing the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # generating bag of words\n",
        "    bag = [0]*len(words)  \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "\n",
        "    return(np.array(bag))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ctFhtopeM6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ERROR_THRESHOLD = 0.30\n",
        "def classify(sentence):\n",
        "    # generate probabilities from the model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    # filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # return tuple of intent and probability\n",
        "    return return_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssjP9qnYeSNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    # if we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        # loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # a random response from the intent\n",
        "                    return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC2-0XvWeU-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46753749-ecb3-4029-9694-11a017e440b8"
      },
      "source": [
        "classify('What are you hours of operation?')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hours', 0.95201164)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_GAoVrHed9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16cd79e0-7287-46cd-8573-181eca3eef9d"
      },
      "source": [
        "response('What are you hours of operation?')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our hours are 9am-9pm every day\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubihMtuMehmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17bc1e2c-4054-40fd-9b7b-9db5686c2575"
      },
      "source": [
        "response('Do you accept Credit Card?')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We accept VISA, Mastercard and AMEX\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7kQHVxxepOp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c517be6-a615-48e2-f408-ef9e54943567"
      },
      "source": [
        "response('That is helpful')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My pleasure\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XecIFiZbe23V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d5a3478-0ef0-4084-cc03-4728d9e65af4"
      },
      "source": [
        "response('Bye')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "See you later, thanks for visiting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO-2LWIie55Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}